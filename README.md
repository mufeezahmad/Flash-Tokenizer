# âš¡ FlashTokenizer (C#)

A high-performance tokenizer engine for large-scale **LLM inference** and **RAG pipelines** in .NET.  
Implements **BERT WordPiece** and **GPT-2 BPE** with a focus on **single-threaded throughput, SIMD optimizations, cache locality, and zero-allocation hot paths**.

Ported and inspired by the upstream [NLPOptimize/flash-tokenizer (C++)](https://github.com/NLPOptimize/flash-tokenizer).

---

## âœ¨ Features

- ðŸ”  **WordPiece (BERT-style)** with forward and bidirectional variants
- ðŸ”¡ **BPE (GPT-2 style)** using `vocab.json` + `merges.txt`
- âš¡ **Span<T>-driven text processing**, `ArrayPool`-backed buffers
- ðŸ§  **Optimized Ahoâ€“Corasick tries** for subword matching
- ðŸš€ **SIMD fast paths** for ASCII casing, whitespace, punctuation
- ðŸ“Š **BenchmarkDotNet integration** for reproducible perf tests
- ðŸ§µ Experimental **async & parallel pipelines**

---

## ðŸ“‚ Repository Structure

```text
csharp/
â”œâ”€â”€ FlashTokenizer/             # âœ… Core library
â”‚   â”œâ”€â”€ WordPieceTokenizer.cs   # WordPiece + bidirectional
â”‚   â”œâ”€â”€ BpeTokenizer.cs         # GPT-2 BPE
â”‚   â”œâ”€â”€ ACTrie.cs               # Flattened Ahoâ€“Corasick DFA
â”‚   â”œâ”€â”€ BasicTokenizer*.cs      # Cleaning, casing, punctuation
â”‚   â”œâ”€â”€ Utf8Util.cs, AccentMap  # Normalization utilities
â”‚   â””â”€â”€ FlashTokenizer.cs       # Public facade + options
â”‚
â”œâ”€â”€ FlashTokenizer.Console/     # ðŸ–¥ Console runner (real file tokenization)
â”‚   â””â”€â”€ Program.cs
â”‚
â”œâ”€â”€ FlashTokenizer.Benchmarks/  # ðŸ“Š BenchmarkDotNet harness
â”‚   â”œâ”€â”€ WordpieceBenchmarks.cs
â”‚   â”œâ”€â”€ TrieBenchmarks.cs
â”‚   â”œâ”€â”€ SimdLatin1Benchmarks.cs
â”‚   â”œâ”€â”€ BasicTokenizerBenchmarks.cs
â”‚   â””â”€â”€ Summary.md
â”‚
â”œâ”€â”€ FlashTokenizer.Tests/       # âœ… Unit & smoke tests
â”‚
â”œâ”€â”€ sample/                     # ðŸ“„ Example vocab + input
â”‚   â””â”€â”€ vocab.txt
â”‚
â””â”€â”€ FlashTokenizer.sln          # ðŸ“¦ Visual Studio 2022 solution
```

## ðŸš€ Quick Start

### Build & Run Console

```bash
dotnet build -c Release
dotnet run -c Release --project csharp/FlashTokenizer.Console
```

# ðŸ›  API Usage

## BERT / WordPiece
```csharp
var tok = new FlashTokenizer(new TokenizerOptions {
    VocabPath = "./sample/vocab.txt",
    DoLowerCase = true,
    Type = TokenizerType.Bert,
});

List<int> ids = tok.Encode("Hello, world!");
string text = tok.Decode(ids);
```

## GPT-2 BPE
```csharp
var tok = new FlashTokenizer(new TokenizerOptions {
    Type = TokenizerType.BPE,
    BpeVocabJsonPath = "./sample/vocab.json",
    BpeMergesPath = "./sample/merges.txt"
});
```

# Benchmarks
Run benchmarks:
```bash
dotnet run -c Release -f net8.0 --project csharp/FlashTokenizer.Benchmarks
```

## ðŸ“ˆ Results Snapshot

Below are sample benchmark results using `BenchmarkDotNet`. These may vary slightly depending on hardware, compiler, and SIMD availability.

| Benchmark                          |   Mean   |  Allocations    |
|------------------------------------|----------|-----------------|
| `WordPiece.TokenizerIdsUtf8`       | ~52 ns   | 184 B/op        |
| `WordPiece.TokenizerIdsString`     | ~137 ns  | 184 B/op        |
| `ACTrie.Search`                    | ~21 ns   | 0 B/op          |
| `Latin1.LowerStrip (SIMD)`         | ~2.7 Âµs  | 1.2 KB/op       |
| `Console (real file, 759k tokens)` | 112 ms | ~4.17M tok/s    |

> ðŸ§ª Benchmarks were run in `Release` mode on .NET 8. SIMD paths auto-detect AVX2/SSE2/AdvSimd.  
> For reproducibility, run:  
> `dotnet run -c Release -f net8.0 --project csharp/FlashTokenizer.Benchmarks`
